{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de Datos (mínimos cuadrados)\n",
    "Profesor: M.en.C. Miguel Ángel Pérez León.\n",
    "\n",
    "Ayudante: Jesús Iván Coss Calderón.\n",
    "\n",
    "Materia: Análisis Numérico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "\n",
    "En estadística y ciencias en general, siempre ha sido una labor importante encontrar una función f que 'ajuste' a un conjunto de datos.\n",
    "\n",
    "Supongamos que como resultado de un experimento se obtienen 4 puntos (x, y) : (1, 6), (2, 5), (3, 7), (4, 10). Y se busca encontrar una recta de la forma\n",
    "\n",
    "$$y=\\alpha+\\beta x \\qquad \\text{(ecuación explícita de la recta)} $$\n",
    "\n",
    "En otras palabras, se buscan valores $\\alpha$, $\\beta$ que se aproximen a solucionar el sistema lineal (sobredeterminado).\n",
    "\n",
    "$$\\begin{cases} \n",
    "\\alpha+1\\beta&=&6\\\\\n",
    "\\alpha+2\\beta&=&5\\\\\n",
    "\\alpha+3\\beta&=&7\\\\\n",
    "\\alpha+4\\beta&=&10\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mínimos Cuadrados\n",
    "\n",
    "La forma de solucionar el problema antes mencionado, se conoce como 'ajuste por mínimos cuadrados'. Y la idea de este algoritmo es tratar de minimizar la suma de los cuadrados de los errores.\n",
    "\n",
    " \n",
    "\n",
    "El error en cada punto entre la curva aproximada y los datos, es la suma del lado izquierdo y derecho de las ecuaciones del sistema $Ax=b$, es decir\n",
    "\n",
    "$F\\left(\\alpha,\\beta\\right)\t=\t\\left[6-\\left(\\alpha+1\\beta\\right)\\right]^{2}+\\left[5-\\left(\\alpha+2\\beta\\right)\\right]^{2}+\\left[7-\\left(\\alpha+3\\beta\\right)\\right]^{2}+\\left[10-\\left(\\alpha+4\\beta\\right)\\right]^{2}\n",
    "\t=\t4\\alpha^{2}+30\\beta^{2}-56\\alpha-154\\beta+20\\alpha\\beta+210$\n",
    "    \n",
    "\n",
    "El mínimo de la función F se determina mediante las derivadas parciales de (también conocido como el gradiente) $F\\left(\\alpha,\\beta\\right)$ igualado a cero\n",
    "\n",
    "$$\\frac{\\delta F}{\\delta\\alpha}=0=8\\alpha+20\\beta-56\\quad y\\quad\\frac{\\delta F}{\\delta\\beta}=0=20\\alpha+60\\beta-154 \\tag{1})$$\n",
    "\n",
    "El sistema (1) es conocido como las **ecuaciones normales** (del sistema original $Ax=b$) debido a que se emplean las derivadas parciales igualadas a cero ¿Que hay de diferente en este nuevo sistema $A'x=b$?.\n",
    "\n",
    "La solución de este sistema es $\\alpha=3.5$ y $\\beta=1.4$, de tal manera que la recta que mejor aproxima linealmente todos los datos de este ejemplo es $$y=3.5+1.4x$$\n",
    "\n",
    "\n",
    "De manera general el problema antes descrito lo podemos expresar de la siguiente forma. Sea $A\\in\\mathbb{R}_{m\\times n}$ y el rango de $A=n$ ( rango(A) renglones linealmente independientes),  $\\vec{x}$,  $\\vec{b}\\in\\mathbb{R}^{n}$.\n",
    "\n",
    "Se tiene un sistema $A\\vec{x}=\\vec{b}$ que no tiene solución (ya que es sobredeterminado), lo que se busca es encontrar los coeficientes $x_{i}$ que hagan que la ecuación buscada se ajuste lo mejor posible en términos de minimización cuadrática.\n",
    "\n",
    "En otras palabras, se trata de solucionar $A\\vec{x}\\approx\\vec{b}$, por lo que se busca un vector $\\hat{x}$ que minimice la función $F\\left(\\vec{x}\\right)$, es decir\n",
    "\n",
    "$$\\hat{x}=min(F(\\vec{x}))$$\n",
    "\n",
    "Como $F(\\vec{x})=\\left\\Vert A\\vec{x}-\\vec{b}\\right\\Vert _{2}^{2}$ y dado que se busca minimizar $F\\left(\\vec{x}\\right)$ se emplea el gradiente ($\\nabla$ operador nabla) igualado a cero.\n",
    "\n",
    "Este problema de minimización tiene solución única ya que el rango de $A$ es $n$, finalmente el nuevo sistema es conocido como las ecuaciones normales que caracterizan al sistema original. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ecuaciones Normales.\n",
    "\n",
    "Una forma de expresar (y mostrar el desarrollo de como llegar) a las ecuaciones normales de un sistema $A\\vec{x}=\\vec{b}$ es empleando el gradiente.\n",
    "\n",
    "Sin embargo existe una forma mas sencilla y compacta de expresar el sistema de ecuaciones normales que caracteriza a dicho sistema.\n",
    "\n",
    "Sea $A\\in\\mathbb{R}_{m\\times n}$ (con $m\\geq n$) y $rango(A)=n$, $b\\in\\mathbb{R}^{m}$ y $\\vec{x}$ la solución que minimiza el residual $r=\\left\\Vert A\\vec{x}-\\vec{b}\\right\\Vert _{2}^{2}$, entonces las ecuaciones normales están dadas por\n",
    "\n",
    "$$A^{T}A\\vec{x}=A^{T}\\vec{b}$$\n",
    "\n",
    "\n",
    "La idea principal para resolver problemas de minimización recae en el concepto de proyección ortogonal. Suponiendo que el vector $\\vec{b}$ no pertenece al subespacio generado por las columnas de la matriz A (o $Im(A)$), denotamos por \n",
    "\n",
    "$$P_{A}:R^{m}\\longrightarrow Im(A)$$\n",
    "\n",
    "A la proyección ortogonal que mapea $R^{m}$ sobre $Im(A)$. Entonces el valor $\\vec{x}$ que minimiza el residual $\\vec{r}=A\\vec{x}-\\vec{b}$ es aquel que satisface $A\\vec{x}=P_{A}\\vec{b}$.\n",
    "\n",
    "\n",
    "En otras palabras el residual $\\vec{r}=A\\vec{x}-\\vec{b}$ debe ser ortogonal al subespacio generado por las columnas de A.\n",
    "\n",
    "![subespacio generado](https://docs.google.com/uc?export=download&id=1rHJ4JiJEVE8kFZGwcl3efzVZf5mnhYwq)\n",
    "\n",
    "\n",
    "**TEOREMA** \n",
    "\n",
    "Sea $A\\in\\mathbb{R}_{m\\times n}$ (con $m\\geq n$) y $rango(A)=n$, $b\\in\\mathbb{R}^{m}$. El residual $\\vec{r}=A\\vec{x}-\\vec{b}$ es ortogonal a $Im(A)$ (subespacio generado por los vectores columna de la matriz) A $\\hspace{2mm}$ si y solo si $\\hspace{2mm}$ $A^{T}A\\vec{x}=A^{T}\\vec{b}$\n",
    "\n",
    "**Demostración.** \n",
    "\n",
    "El residual $\\vec{r}$ es ortogonal a $Im(A)$ si y solo si $\\vec{a}_{i}^{T}\\vec{r}=0$ $\\forall i=1,\\ldots,n$. Por lo tanto $\\vec{r}$ es ortogonal a $Im(A)$ si y solo si\n",
    "\n",
    "$$A^{T}\\left(A\\vec{x}-\\vec{b}\\right)=\\vec{0} \\tag{2} $$\n",
    "\n",
    "Pero de (2) tenemos que\n",
    "\n",
    "$$A^{T}\\left(A\\vec{x}-\\vec{b}\\right)=A^{T}A\\vec{x}-A^{T}\\vec{b}=0\\Longrightarrow A^{T}A\\vec{x}=A^{T}\\vec{b}$$\n",
    "\n",
    "Por lo tanto $\\vec{r}$ es ortogonal a $Im(A)$ si y solo si\n",
    "\n",
    "$$A^{T}A\\vec{x}=A^{T}\\vec{b}$$\n",
    "\n",
    "Encuentre las ecuaciones normales del sistema $A\\vec{x}=\\vec{b}$ y encuentre la ecuación de la recta que se ajusta de mejor manera a los puntos: $\\left(x,y\\right)$: $ \\color{red}{(1,6),(2,5),(3,7),(4,10)}$ \n",
    "\n",
    "$\\color{blue}{Solución}$: Dados los puntos (x,y) de este ejemplo, se busca minimizar la función que pase por todos ellos entonces el sistema $A\\vec{x}=\\vec{b}$ es el siguiente\n",
    "\n",
    "$$A\\vec{x}=\\left(\\begin{array}{cc}\n",
    "1 & 1\\\\\n",
    "1 & 2\\\\\n",
    "1 & 3\\\\\n",
    "1 & 4\n",
    "\\end{array}\\right)\\left(\\begin{array}{c}\n",
    "\\alpha\\\\\n",
    "\\beta\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "6\\\\\n",
    "5\\\\\n",
    "7\\\\\n",
    "10\n",
    "\\end{array}\\right) $$\n",
    "\n",
    "Así que empleando el teorema antes mencionado, las ecuaciones normales de este sistema son $A^{T}A\\vec{x}=A^{T}\\vec{b}$\n",
    "\n",
    "$$\\left[\\left(\\begin{array}{cccc}\n",
    "1 & 1 & 1 & 1\\\\\n",
    "1 & 2 & 3 & 4\n",
    "\\end{array}\\right)\\left(\\begin{array}{cc}\n",
    "1 & 1\\\\\n",
    "1 & 2\\\\\n",
    "1 & 3\\\\\n",
    "1 & 4\n",
    "\\end{array}\\right)\\right]\\left(\\begin{array}{c}\n",
    "\\hat{x}_{1}\\\\\n",
    "\\hat{x}_{2}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{cccc}\n",
    "1 & 1 & 1 & 1\\\\\n",
    "1 & 2 & 3 & 4\n",
    "\\end{array}\\right)\\left(\\begin{array}{c}\n",
    "6\\\\\n",
    "5\\\\\n",
    "7\\\\\n",
    "10\n",
    "\\end{array}\\right) $$\n",
    "\n",
    "Al resolver este nuevo sistema $(A\\hat{x}=\\vec{b})$ se obtiene que el vector solución es $\\left(3.5,1.4\\right)$, así que la recta que mejor se ajusta a los datos de este ejemplo es\n",
    "\n",
    "$$ y=3.5+1.4x$$\n",
    "\n",
    "!Compruébalo con $\\color{green}{Python}$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Factorización QR.\n",
    "\n",
    "Como ya se ha visto a lo largo de este curso, es recurrente tratar de resolver un sistema del tipo $A\\vec{x}=\\vec{b}$ y dado que el cálculo de la matriz inversa de $A$ es un proceso costoso, se buscan alternativas.\n",
    "\n",
    "La matriz asociada al sistema de ecuaciones normales $\\left(A^{T}A\\right)$ vista en la sección anterior, tiene peculiares características ($\\color{blue}{Simétrica y Positiva Definida}$) que la convierte en candidato a diferentes tipos de factorizaciones, en este caso la factorización QR.\n",
    "\n",
    "**HECHO**\n",
    "\n",
    "Una matriz $A\\in\\mathbb{R}_{n\\times n}$\n",
    "\n",
    "1. Es simétrica si: $A=A^{T}$\n",
    "\n",
    "2. Es positiva definida si: Todos sus determinantes superiores izquierdos de $i\\times i,\\quad i=1,\\ldots,n$ son positivos. \n",
    "\n",
    "\n",
    "**Teorema**\n",
    "\n",
    "Sea $A\\in\\mathbb{R}_{m\\times n$} (con $m\\geq n$) y $rango\\left(A\\right)=n$ existe una única factorización QR tal que\n",
    "\n",
    "$$A=QR\\quad con \\quad r_{ii}>0\\quad i=1,\\ldots,n$$\n",
    "\n",
    "### Método de Gram-Schmidt.\n",
    "\n",
    "La idea detrás de la factorización QR es construir una base ortonormal $\\left(q_{1},q_{2},\\ldots,q_{n}\\in R^{m}\\right)$ para el subespacio generado por los vectores columna de A.\n",
    "\n",
    "*  Se comienza utilizando el primer vector columna de $A\\quad(a_{1})$. Es decir $q_{1}=\\frac{a_{1}}{\\left\\Vert a_{1}\\right\\Vert _{2}}$.\n",
    "\n",
    "\n",
    "* A continuación se substrae del vector $a_{2}$ su coeficiente en la dirección de $q_{1}$, es decir $\\left(q_{1}^{T}\\cdot a_{1}\\right)\\cdot q_{1}$. Dando como resultado un vector $q_{2}$, que es ortogonal a $q_{1}$. Finalmente se normaliza el vector $q_{2}$ para volverlo ortonormal.\n",
    "\n",
    "\n",
    "* El proceso se repite con el resto de los vectores columna de $A=\\left[a_{1},a_{2},\\ldots,a_{n}\\right]$\n",
    "\n",
    "\n",
    "En términos matemáticos, el proceso se vería de la siguiente manera.\n",
    "\n",
    "### Algoritmo.\n",
    "\n",
    "1. $q_{1}=\\frac{v_{1}}{\\left\\Vert v_{1}\\right\\Vert _{2}}\\quad con\\quad v_{1}=a_{1}$\n",
    "\n",
    "2. $ q_{2}=\\frac{v_{2}}{\\left\\Vert v_{2}\\right\\Vert _{2}}\\quad con\\quad v_{2}=a_{2}-\\left(q_{1}^{T}\\cdot a_{2}\\right)\\cdot q_{1} $\n",
    "\n",
    "3. $q_{3}=\\frac{v_{3}}{\\left\\Vert v_{3}\\right\\Vert _{2}}\\quad con\\quad v_{3}=a_{3}-\\left(q_{1}^{T}\\cdot a_{3}\\right)\\cdot q_{1}-\\left(q_{2}^{T}\\cdot a_{3}\\right)\\cdot q_{2} $\n",
    "\n",
    "\n",
    "4. En general el j-esimo paso, suponiendo conocidos $q_{1},\\ldots,q_{j-1}$, es un vector $q_{j}$ ortonormal a todos los vectores ya conocidos, es decir\n",
    "\n",
    "$$ q_{j}=\\frac{v_{j}}{\\left\\Vert v_{j}\\right\\Vert _{2}}\\quad con\\quad v_{j}=a_{j}-\\left(q_{1}^{T}\\cdot a_{j}\\right)\\cdot q_{1}-\\cdots-\\left(q_{j-1}^{T}\\cdot a_{j}\\right)\\cdot q_{j-1}$$\n",
    "\n",
    "\n",
    "5. En forma de suma, el vector $v_{j}$ se ve de la siguiente manera\n",
    "\n",
    "$$v_{j}=a_{j}-\\sum_{i=1}^{j-1}\\left(q_{i}^{T}\\cdot a_{j}\\right)\\cdot q_{i}$$\n",
    "\n",
    "\n",
    "$\\color{blue}{EJEMPLO.}$\n",
    "\n",
    "\n",
    "$Sea \\quad A=\\left(\\begin{array}{ccc}\n",
    "-1 & -1 & 1\\\\\n",
    "1 & 3 & 3\\\\\n",
    "-1 & -1 & 5\\\\\n",
    "1 & 3 & 7\n",
    "\\end{array}\\right)$ encuentre su factorización $A=QR$ \n",
    "\n",
    "$\\color{blue}{Solución:}$\n",
    "\n",
    "\n",
    "$$q_{1}=\\frac{v_{1}}{\\left\\Vert v_{1}\\right\\Vert _{2}}\\quad v_{1}=a_{1}=\\left(\\begin{array}{c}\n",
    "-1\\\\\n",
    "1\\\\\n",
    "-1\\\\\n",
    "1\n",
    "\\end{array}\\right)\\quad{\\color{red}r}_{{\\color{red}{11}}}=\\left\\Vert v_{1}\\right\\Vert _{2}=2\\Longrightarrow{\\color{red}{q_{1}=\\frac{v_{1}}{r_{11}}=\\left(\\begin{array}{c}\n",
    "-\\frac{1}{2}\\\\\n",
    "\\frac{1}{2}\\\\\n",
    "-\\frac{1}{2}\\\\\n",
    "\\frac{1}{2}\n",
    "\\end{array}\\right)}}\\quad{\\color{red}r}_{{\\color{red}{12}}}=q_{1}^{T}a_{2}=4 $$\n",
    "\n",
    "\n",
    "$$v_{2}=a_{2}-r_{12}q_{1}=\\left(\\begin{array}{c}\n",
    "-1\\\\\n",
    "3\\\\\n",
    "-1\\\\\n",
    "3\n",
    "\\end{array}\\right)-4\\left(\\begin{array}{c}\n",
    "-\\frac{1}{2}\\\\\n",
    "\\frac{1}{2}\\\\\n",
    "-\\frac{1}{2}\\\\\n",
    "\\frac{1}{2}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "1\\\\\n",
    "1\\\\\n",
    "1\\\\\n",
    "1\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "\n",
    "$${\\color{red}r}_{{\\color{red}{22}}}=\\left\\Vert v_{2}\\right\\Vert _{2}=2\\Longrightarrow{\\color{red}{q_{2}=\\frac{v_{2}}{r_{22}}=\\left(\\begin{array}{c}\n",
    "\\frac{1}{2}\\\\\n",
    "\\frac{1}{2}\\\\\n",
    "\\frac{1}{2}\\\\\n",
    "\\frac{1}{2}\n",
    "\\end{array}\\right)}}$$\n",
    "\n",
    "$${\\color{red}r}_{{\\color{red}{13}}}=q_{1}^{T}a_{3}=2 \\quad y \\quad {\\color{red}r}_{{\\color{red}{23}}}=q_{2}^{T}a_{3}=8$$\n",
    "\n",
    "\n",
    "$$v_{3}=a_{3}-r_{13}q_{1}-r_{23}q_{2}=\\left(\\begin{array}{c}\n",
    "1\\\\\n",
    "3\\\\\n",
    "5\\\\\n",
    "7\n",
    "\\end{array}\\right)-2\\left(\\begin{array}{c}\n",
    "-\\frac{1}{2}\\\\\n",
    "\\frac{1}{2}\\\\\n",
    "-\\frac{1}{2}\\\\\n",
    "\\frac{1}{2}\n",
    "\\end{array}\\right)-8\\left(\\begin{array}{c}\n",
    "\\frac{1}{2}\\\\\n",
    "\\frac{1}{2}\\\\\n",
    "\\frac{1}{2}\\\\\n",
    "\\frac{1}{2}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "-2\\\\\n",
    "-2\\\\\n",
    "2\\\\\n",
    "2\n",
    "\\end{array}\\right)$$\n",
    "\n",
    "\n",
    "$${\\color{red}r}_{{\\color{red}{33}}}=\\left\\Vert v_{3}\\right\\Vert _{2}=4\\Longrightarrow{\\color{red}{q_{3}=\\frac{v_{2}}{r_{33}}=\\left(\\begin{array}{c}\n",
    "-\\frac{1}{2}\\\\\n",
    "-\\frac{1}{2}\\\\\n",
    "\\frac{1}{2}\\\\\n",
    "\\frac{1}{2}\n",
    "\\end{array}\\right)} }$$\n",
    "\n",
    "**Por lo tanto**\n",
    "\n",
    "$$ A=\\left(\\begin{array}{ccc}\n",
    "-1 & -1 & 1\\\\\n",
    "1 & 3 & 3\\\\\n",
    "-1 & -1 & 5\\\\\n",
    "1 & 3 & 7\n",
    "\\end{array}\\right)=\\left(\\begin{array}{ccc}\n",
    "q_{1} & q_{2} & q_{3}\\end{array}\\right)\\left(\\begin{array}{ccc}\n",
    "r_{11} & r_{12} & r_{13}\\\\\n",
    "0 & r_{22} & r_{23}\\\\\n",
    "0 & 0 & r_{33}\n",
    "\\end{array}\\right)\\left(\\begin{array}{ccc}\n",
    "-1 & -1 & 1\\\\\n",
    "1 & 3 & 3\\\\\n",
    "-1 & -1 & 5\\\\\n",
    "1 & 3 & 7\n",
    "\\end{array}\\right)=\\left(\\begin{array}{ccc}\n",
    "-\\frac{1}{2} & \\frac{1}{2} & -\\frac{1}{2}\\\\\n",
    "\\frac{1}{2} & \\frac{1}{2} & -\\frac{1}{2}\\\\\n",
    "-\\frac{1}{2} & \\frac{1}{2} & \\frac{1}{2}\\\\\n",
    "\\frac{1}{2} & \\frac{1}{2} & \\frac{1}{2}\n",
    "\\end{array}\\right)\\left(\\begin{array}{ccc}\n",
    "2 & 4 & 2\\\\\n",
    "0 & 2 & 8\\\\\n",
    "0 & 0 & 4\n",
    "\\end{array}\\right) $$\n",
    "\n",
    "¡Compruébalo con $\\color{green}{Python}$!\n",
    "\n",
    "### Propiedades de QR \n",
    "\n",
    "La factorización $QR$ implica que la matriz $Q$ es una matriz ortonormal y la matriz $R$ es una matriz triangular superior. Estos factores tiene muchas ventajas.\n",
    "\n",
    "**Hecho.** \n",
    "\n",
    "Sea $A\\in\\mathbb{R}_{n\\times n}$\n",
    "\n",
    "1. Si $Q$ es ortogonal entonces $Q^{T}Q=Id$\n",
    "\n",
    "2. $QR=A\\Longrightarrow R=Q^{T}A.$\n",
    "\n",
    "3. Dado que de las ecuaciones normales definidas por $A^{T}A\\vec{x}=A^{T}\\vec{b}$, garantizan que $A^{T}A$ es una matriz positiva definida, entonces $A^{T}A$ se pude factorizar en $QR$.\n",
    "\n",
    "4. Sea $A'=A^{T}A$ y $b'=A^{T}\\vec{b}$. Entonces\n",
    "\n",
    "$$A'\\vec{x}=b'\\Longrightarrow QR\\vec{x}=b'\\Longrightarrow Q^{T}QR\\vec{x}=Q^{T}b' \\Longrightarrow R\\vec{x}=Q^{T}b'$$\n",
    "\n",
    "Y resulta que es un sistema triangular que puede ser resuelto fácilmente mediante su buen amigo, substitución hacia atrás.\n",
    "\n",
    "![Algoritmo QR](https://docs.google.com/uc?export=download&id=1gkXr1zspatPkDBVh079v3D0K2dVvNlmc)\n",
    "\n",
    "### Procedimiento general para Mínimos Cuadrados.\n",
    "\n",
    "Ya que se entendido el por que es útil entender el proceso de mínimos cuadrados y como es que las factorizaciones ayudan a resolverlo, ahora lo que resta es automatizar todo el proceso que lleva a resolver el problema general, es decir.\n",
    "\n",
    "* Se tiene una lista de puntos de la forma $\\left(x,y\\right)$, donde cada punto representa datos reales de algún fenómeno (estadística, genética, geofísica, física biomédica, etc).\n",
    "\n",
    "\n",
    "* A continuación se encuentran las ecuaciones normales del sistema empleando la igualdad $A^{T}A\\vec{x}=A^{T}\\vec{b}=b'.$\n",
    "\n",
    "\n",
    "* Aplicamos alguna factorización, por ejemplo Cholesky $A^{T}A=A\\text{'}=L^{T}L$ ó $A\\text{'}=QR$ empleando el proceso de Gram-Schmidt para encontrar una base ortonormal para el subespacio generado por los vectores columna de A'.\n",
    "\n",
    "\n",
    "* Aplicando las propiedades de $L^{T}$ y $L$ ó $Q$ y $R$ se resuelve el nuevo sistema (mediante substitución hacia atrás), es decir $QR\\vec{x}=b'$ $\\Longrightarrow$  $R\\vec{x}=Q^{T}b'$.\n",
    "\n",
    "\n",
    "* Una vez encontrados los coeficientes $\\alpha$ y $\\beta$ de la recta que minimizan los errores cuadráticos, ahora es posible graficar dicha recta e incluso interpolar.\n",
    "\n",
    "¡Intentalo con $\\color{green}{Python}$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Referencias.\n",
    "\n",
    "Justin Solomon: Numerical Algorithms.\n",
    "\n",
    "Jaan Kiusalaas: Numerical Methods in Engineering with Python.\n",
    "\n",
    "Richard L. Burden, J. Douglas Faires: Análisis Numérico, Math Learning.\n",
    "\n",
    "Riswan Butt: Numerical Analysys Using Matlab, Jones and Bartlett.\n",
    "\n",
    "Ward Cheney, David Kincaid.: Métodos Numéricos y Computación, Cenage Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
