{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_RedesNeuronales.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMHCvQynzmlq71q0aTJSIpn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugernaut/Numerico2021/blob/desarrollo/RedesNeuronales/02_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXSdMY-R3hny"
      },
      "source": [
        "<font color=\"Teal\" face=\"Comic Sans MS,arial\">\n",
        "  <h1 align=\"center\"><i>Perceptrón</i></h1>\n",
        "  </font>\n",
        "  <font color=\"Black\" face=\"Comic Sans MS,arial\">\n",
        "  <h5 align=\"center\"><i>Profesor: M.en.C. Miguel Ángel Pérez León."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdSUVg0fNCxN"
      },
      "source": [
        "# Introducción\n",
        "\n",
        "El algoritmo perceptrón fue publicado en 1957 por **Frank Rosenblatt**.\n",
        "\n",
        "El objetivo del perceptrón es encontrar un **hiperplano** capaz de separar correctamente un conjunto de datos que sean **linealmente separables**, una vez obtenido el hiperplano, este puede utilizarse para **clasificaciones binarias**.\n",
        "\n",
        "Aunque el perceptrón es un algoritmo de aprendizaje muy simple, entender su funcionamiento es clave para aprender otros métodos más complejos como las **redes neuronales artificiales**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhcAIUHQlTzx"
      },
      "source": [
        "## Funcionamiento\n",
        "\n",
        "El funcionamiento del perceptrón es muy sencillo, simplemente lee los valores de entrada, suma todos las entradas de acuerdo a unos pesos (**suma ponderada**) más el **sesgo** y el resultado lo introduce en una **función de activación** que genera el resultado final.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K6SXEU7pyiD"
      },
      "source": [
        "## Entrenamiento (aprendizaje)\n",
        "\n",
        "El entrenamiento del perceptrón **no es más que determinar los pesos sinápticos y el sesgo** que mejor hagan que la entrada se ajuste a la salida.\n",
        "\n",
        "Para la determinación de estas variables, se sigue un proceso adaptativo.\n",
        "\n",
        "El proceso  comienza con valores aleatorios y se van modificando estos valores según la diferencia entre los valores deseados y los calculados por el perceptrón."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or2MGaOupukJ"
      },
      "source": [
        "## Algoritmo\n",
        "\n",
        "En forma de algoritmo, el procesos de aprendizaje del perceptrón es el siguiente:\n",
        "\n",
        "\n",
        "\n",
        "1.   Inicializar pesos y sesgo.\n",
        "2.   Ciclo: hasta resultado de pesos y sesgo sea aceptable.\n",
        "3.   Ciclo: para todos los ejemplos de entrenamiento.\n",
        "4.   Leer valores de entrada.\n",
        ">5. Calcular error.\n",
        ">6. Actualizar pesos según el error.\n",
        ">7. Actualizar el sesgo.\n",
        ">8. Una vez encontrados los pesos y sesgos que **minimizan el error**, termina el algoritmo.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/desarrollo/Figuras/redesNeuronales/red.gif?raw=1\" width=\"450\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv34b5WB2PKL"
      },
      "source": [
        "#  Perceptrón AND \n",
        "\n",
        "Vamos a construir desde el comienzo un perceptrón que sea capás de aprender como funciona el conectivo lógico \"y\" (and, &).\n",
        "\n",
        "Este conectivo lo usamos diariamente y esta es su tabla de verdad.\n",
        "\n",
        "| X1 | X2 | Salida| \n",
        "| :-: | :-: | :-: |\n",
        "| $0$ | $0$ | $0$ |\n",
        "| $0$ | $1$ | $0$ |\n",
        "| $1$ | $0$ | $0$ |\n",
        "| $1$ | $1$ | $1$ |\n",
        "\n",
        "Un ejemplo sencillo de como se usa puede ser el siguiente. *Hoy es viernes y estoy en mi el taller de fundación Telmex*, dado que ambas sentencias son verdaderas y están conectadas por el conectivo lógico \"y\" entonces el valor de **este enunciado es verdadero**. En caso de que no sea viernes o de que no me encuentre en el taller de fundación Telmex, el valor de este **enunciado seria falso**, como lo muestra la tabla de verdad.\n",
        "\n",
        "Si pensamos lo anterior en términos de una neurona artificial, esta debería verse así.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/jugernaut/Numerico2021/blob/desarrollo/Figuras/redesNeuronales/and.png?raw=1\" width=\"750\">\n",
        "</center>\n",
        "\n",
        "Es de notar que las entradas son variables que va a recibir nuestro perceptrón y lo que necesitamos \"ajustar\" entonces son los **pesos** y el **sesgo** y la pregunta evidente es ¿cómo lo hacemos?.\n",
        "\n",
        "Una de las formas más sencillas es mediante prueba y error, el problema con esté método es que nos tomaría demasiado tiempo y para problemas con más entradas sería prácticamente imposible usar tal estrategia.\n",
        "\n",
        "Así que aquí surge la idea de **minimizar el error**, algo muy similar al proceso de mínimos cuadrados o regresión logística."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AQHpP046Z6_"
      },
      "source": [
        "#  Referencias\n",
        "\n",
        "*   [Prometeo](https://github.com/jugernaut/Prometeo)\n",
        "*   [Perceptron](https://medium.com/@thomascountz/19-line-line-by-line-python-perceptron-b6f113b161f3)\n",
        "*   [Brilliant](https://brilliant.org/practice/intelligent-computers-menace/?chapter=introduction-to-neural-networks)\n",
        "*   [Simulador](https://ml4a.github.io/ml4a/es/neural_networks/)\n",
        "*   [NetLogo](https://ccl.northwestern.edu/netlogo/)\n",
        "*   [Red neuronal desde cero](https://futurelab.mx/redes%20neuronales/inteligencia%20artificial/2019/06/25/intro-a-redes-neuronales-pt-1/)\n",
        "*   [Libro Web](http://neuralnetworksanddeeplearning.com/index.html)\n",
        "*   [Aprendizaje profundo](https://www.deeplearningbook.org/)\n",
        "*   [Lista de videos](https://www.youtube.com/playlist?list=PLo8YL3HL50lUHQS80oE_ypxFi0Y3uCVal)\n",
        "*   [Algebra Lineal](https://www.youtube.com/playlist?list=PLIb_io8a5NB2DddFf-PwvZDCOUNT1GZoA)\n"
      ]
    }
  ]
}